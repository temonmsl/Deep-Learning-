# 非線性激活函數的目的

非線性激活函數的主要目的是為神經網絡模型引入**非線性能力**，從而幫助它學習覆雜的模式和關系。以下是詳細的目的和原因：

## 1. 引入非線性特性

如果沒有非線性激活函數，神經網絡的每一層只是輸入的線性組合，最終整個網絡等效於線性模型。線性模型無法處理覆雜問題，如分類和圖像識別。非線性激活函數允許網絡學習更覆雜的關系。

## 2. 增加模型表達能力

非線性激活函數允許神經網絡更好地學習輸入和輸出之間的覆雜映射關系。通過引入非線性，神經網絡可以逐層積累不同的特征，從而提升其表達能力。

## 3. 解決梯度消失問題

某些激活函數（如 ReLU）能夠有效避免梯度消失問題，這使得網絡在反向傳播時能夠保持梯度，特別是在深層網絡中，梯度不會迅速消失。

## 4. 提升模型性能

非線性激活函數幫助網絡學到更具區分性的特征，在圖像分類、語音識別等任務中表現更好。例如，ReLU 激活函數使得網絡可以更好地提取出關鍵特征。

---

## 常見的非線性激活函數

- **ReLU（Rectified Linear Unit）**
  - 優點：簡單、計算效率高，避免梯度消失。
  - 缺點：可能出現“死亡神經元”問題。
- **Sigmoid**
  - 優點：用於輸出概率值，輸出值在 (0, 1) 之間。
  - 缺點：容易造成梯度消失，訓練深層網絡時不穩定。
- **Tanh**
  - 優點：輸出範圍 [-1, 1]，在一些情況下比 Sigmoid 表現更好。
  - 缺點：同樣可能導致梯度消失。
- **Leaky ReLU**
  - 優點：緩解了 ReLU 的“死亡神經元”問題，負數部分有較小斜率。
  - 缺點：需要對負斜率手動調參。

---

## 總結

非線性激活函數是神經網絡不可或缺的一部分，它使模型具備處理非線性問題的能力，能夠學習覆雜的特征，從而提升性能。
